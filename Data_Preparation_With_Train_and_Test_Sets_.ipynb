{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tincorpai/tincorpai-Data-Preparation-and-Cleaning-in-Machine-Learning-/blob/master/Data_Preparation_With_Train_and_Test_Sets_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a239e7a7",
      "metadata": {
        "id": "a239e7a7"
      },
      "source": [
        "## Data Preparation With Train and Test Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "367e4697",
      "metadata": {
        "id": "367e4697"
      },
      "source": [
        "Evaluate logistic regression model using train and test sets on a synthetic binary classification dataset where the input variables have been normalized. \n",
        "\n",
        "> The next is to define the synthetic dataset. Use make_classification() function to create the dataset with 1000 rows of data and 20 numerical input features. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb6ef23b",
      "metadata": {
        "id": "eb6ef23b"
      },
      "outputs": [],
      "source": [
        "# test classification dataset\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "#define dataset\n",
        "X, y = make_classification(n_samples = 1000, n_features = 20, n_informative = 15, n_redundant=5, random_state=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd3826d1",
      "metadata": {
        "id": "fd3826d1",
        "outputId": "3a66fc3c-48c6-420c-bcbe-ec86a0a13605"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1000, 20) (1000,)\n"
          ]
        }
      ],
      "source": [
        "#Summarize the dataset\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "836b1aa9",
      "metadata": {
        "id": "836b1aa9"
      },
      "source": [
        "We now evaluate our model on a scaled dataset scale using the naive and incorrect approach"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11e1ee89",
      "metadata": {
        "id": "11e1ee89"
      },
      "source": [
        "## Train-Test Evaluation With Naive Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e8eab80",
      "metadata": {
        "id": "8e8eab80"
      },
      "source": [
        "This approach involves applying the data preparation technique to the entire dataset - both the training dataset and testing set. This technique will definitely result in data leakage.\n",
        "\n",
        "> We can scale all features in the dataset to the range 0-1, then use the fit_transform() function to fit the transform the transform on the dataset in a single step. The result is a normalized version of the input variables, where each column in the array is normalized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f3bedd9",
      "metadata": {
        "id": "5f3bedd9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "# define data\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7c774ea",
      "metadata": {
        "id": "f7c774ea"
      },
      "source": [
        "Next, split the dataset into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a47da12",
      "metadata": {
        "id": "8a47da12"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state = 1)"
      ]
    },
    {
      "cell_type": "raw",
      "id": "ba108191",
      "metadata": {
        "id": "ba108191"
      },
      "source": [
        "We can define our logistic regression algorthm via the LogisticRegression class, with the default configuration, and fit it on the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c488531b",
      "metadata": {
        "id": "c488531b",
        "outputId": "ead8f33c-5680-4b4b-df37-21243369c527"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# fit the model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dfa14c7",
      "metadata": {
        "id": "5dfa14c7"
      },
      "source": [
        "We can then make a prediction using the input data from the test set, and we can compare the predictions to the expected values and calculate a classification accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da1482a4",
      "metadata": {
        "id": "da1482a4",
        "outputId": "9d8bfd41-a57f-4d7f-97eb-fac73a06770e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 84.848\n"
          ]
        }
      ],
      "source": [
        "#evaluate the model\n",
        "y_hat = model.predict(X_test)\n",
        "# evaluate predictions\n",
        "accuracy = accuracy_score(y_test, y_hat)\n",
        "print(\"Accuracy: %.3f\" % (accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0e0f5b0",
      "metadata": {
        "id": "f0e0f5b0"
      },
      "source": [
        "Now let's look how to avoid data leakage "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c006444b",
      "metadata": {
        "id": "c006444b"
      },
      "source": [
        "The correct approach to performing data preparation with the train-test split evaluation is to fit the data preparation on the training set, then apply the transform to the train and test sets. This requires that we first fit the dataset into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "205e78f5",
      "metadata": {
        "id": "205e78f5"
      },
      "outputs": [],
      "source": [
        "X, y = make_classification(n_samples = 1000, n_features = 20, n_informative = 15, n_redundant=5, random_state=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "022ebf4e",
      "metadata": {
        "id": "022ebf4e"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "434b5040",
      "metadata": {
        "id": "434b5040"
      },
      "source": [
        "We can then  define the MinMaxScaler and call the fit() function on the training set. Thereafter, apply the transform() function on the train and test sets to create a normalized version of each dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e07cf8c",
      "metadata": {
        "id": "7e07cf8c"
      },
      "outputs": [],
      "source": [
        "# define the scaler\n",
        "scaler = MinMaxScaler()\n",
        "# fit on the training dataset\n",
        "scaler.fit(X_train)\n",
        "# scale the training dataset\n",
        "X_train = scaler.transform(X_train)\n",
        "# scale the test dataset\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56bbb3f8",
      "metadata": {
        "id": "56bbb3f8"
      },
      "source": [
        "This is an example of fitting the train set and applying it to both train and test sets. This type of method avoids data leakage as the calculation of the manimum and maximum value for each input variable is calculated using the training set instead of the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1072b8d8",
      "metadata": {
        "id": "1072b8d8",
        "outputId": "6724b441-3100-4f1a-ddd9-31757a7ec7cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 85.455\n"
          ]
        }
      ],
      "source": [
        "# fit the model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "# evaluate the model\n",
        "yhat = model.predict(X_test)\n",
        "# evaluate predictions\n",
        "accuracy = accuracy_score(y_test, yhat)\n",
        "print('Accuracy: %.3f' % (accuracy*100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ed5f1aa",
      "metadata": {
        "id": "1ed5f1aa"
      },
      "source": [
        "We can see this method of transforming only the X_train dataset give accurate prediction (85.455 percent) compare to the previous method (84.848 percent). We expect that data leakage to result in an incorrect estimate of the model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e0956f4",
      "metadata": {
        "id": "6e0956f4"
      },
      "source": [
        "## Data Preparation With k-fold Cross-Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d8a9f82",
      "metadata": {
        "id": "6d8a9f82"
      },
      "source": [
        "The k-fold cross-validation involves splitting a dataset into k non-overlapping groups evaluated on the held-out fold. This process is repeated so that each fold is given a chance to be used as the holdout test set. The average performance across all evaluations is reported.\n",
        "\n",
        "The k-fold cross-validation procedure gives a more reliable estimate of model performance than a trian-test split, although it is more computationally expensive the repeated fitting and evaluation of models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a1c40e2",
      "metadata": {
        "id": "3a1c40e2"
      },
      "source": [
        "## Cross-Validation Evaluation With Naive Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7c955c5",
      "metadata": {
        "id": "e7c955c5"
      },
      "source": [
        "The naive data preparation with cross-validation involves applying the data transforms first, then using the cross-validation procedure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b24853a",
      "metadata": {
        "id": "0b24853a"
      },
      "outputs": [],
      "source": [
        "X, y = make_classification(n_samples = 1000, n_features = 20, n_informative = 15, n_redundant=5, random_state=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "445cfa67",
      "metadata": {
        "id": "445cfa67"
      },
      "outputs": [],
      "source": [
        "# standardize the dataset\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bad2968",
      "metadata": {
        "id": "3bad2968"
      },
      "source": [
        "The k-fold cross-validation procedure must first be defined. We will use repeated stratified 10-fold cross-validation, which a best practice for classification.Repeated means that the\n",
        "whole cross-validation procedure is repeated multiple times, three in this case. Stratified means\n",
        "that each group of rows will have the relative composition of examples from each class as the\n",
        "whole dataset. We will use k = 10 or 10-fold cross-validation. This can be achieved using the\n",
        "RepeatedStratifiedKFold which can be configured to three repeats and 10 folds, and then\n",
        "using the cross val score() function to perform the procedure, passing in the defined model,\n",
        "cross-validation object, and metric to calculate, in this case, accurac"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "611330af",
      "metadata": {
        "id": "611330af",
        "outputId": "490f6c46-30c5-44fa-8078-b8db688a052c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 85.300 (3.607)\n"
          ]
        }
      ],
      "source": [
        "# naive data preparation for model evaluation with k-fold cross-validation\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\n",
        "random_state=7)\n",
        "# standardize the dataset\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "# define the model\n",
        "model = LogisticRegression()\n",
        "# define the evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate the model using cross-validation\n",
        "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eecc76f2",
      "metadata": {
        "id": "eecc76f2"
      },
      "source": [
        "In this case, we can see that the model achieved an estimated accuracy of about 8.300 percent. Which we know is incorrect given the data leakage allowed via data preparation procedure."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be54dde7",
      "metadata": {
        "id": "be54dde7"
      },
      "source": [
        "Now, let's evaluate the model with cross-validation and avoid data leakage."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c73d4fe2",
      "metadata": {
        "id": "c73d4fe2"
      },
      "source": [
        "## Cross Validation Evaluation With Correct Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fc88fd4",
      "metadata": {
        "id": "1fc88fd4"
      },
      "source": [
        "The evaluation procedure changes from simply and incorrectly evaluating just the model\n",
        "to correctly evaluating the entire pipeline of data preparation and model together as a single\n",
        "atomic unit. This can be achieved using the Pipeline class. This class takes a list of steps\n",
        "that define the pipeline. Each step in the list is a tuple with two elements. The first element is\n",
        "the name of the step (a string) and the second is the configured object of the step, such as a\n",
        "transform or a model. The model is only supported as the final step, although we can have as\n",
        "many transforms as we like in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6d85f42",
      "metadata": {
        "id": "a6d85f42",
        "outputId": "7e08f8bc-1d5d-455c-f67b-a916f41f29b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 85.433 (3.471)\n"
          ]
        }
      ],
      "source": [
        "# correct data preparation for model evaluation with k-fold cross-validation\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\n",
        "random_state=7)\n",
        "# define the pipeline\n",
        "steps = list()\n",
        "steps.append(('scaler', MinMaxScaler()))\n",
        "steps.append(('model', LogisticRegression()))\n",
        "pipeline = Pipeline(steps=steps)\n",
        "# define the evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate the model using cross-validation\n",
        "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cb8cfe0",
      "metadata": {
        "id": "2cb8cfe0"
      },
      "source": [
        "Running the example normalizes the data correctly within the cross-validation folds of the evaluation procedure to avoid data leakage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eb624a7",
      "metadata": {
        "id": "1eb624a7"
      },
      "source": [
        "We have seen here that there is an improvement in the predictive accuracy from 85.300 percent to about 85.433 percent. This example demonstrate that data leakage may impact the estimate of model performance and how to correct data leakage by correctly perform data preparation after the data is split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da653cd7",
      "metadata": {
        "id": "da653cd7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}